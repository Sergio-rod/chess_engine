{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f738d7b1-089b-4b95-8723-97fbba9e189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d0072c-c552-4f19-9aea-cb5a9f058494",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets = glob.glob('../data/processed/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f298a1-4c7a-433d-90eb-26e2296cfd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_million_games = pd.concat([pd.read_parquet(parquet) for parquet in parquets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9a2093-8711-4726-8647-995475b26ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_uci_move_vocabulary() -> tuple[dict,dict]:\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a set of all the posible movements in a chess board of 64 squares, \n",
    "    create two dictionaries which will represent the board move in uci format and their respective idx value and viceversa\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        uci_to_idx : dict\n",
    "                     All the possible uci moves in a chess board, uci format as keys and idx as values\n",
    "        idx_to_uci : dict \n",
    "                     All the possible uci moves in a chess board, uci format as keys and idx as values\n",
    "    \n",
    "    \"\"\"\n",
    "    move_set = set()\n",
    "    \n",
    "    for from_sq in chess.SQUARES:\n",
    "        for to_sq in chess.SQUARES:\n",
    "            if from_sq == to_sq:\n",
    "                continue\n",
    "\n",
    "            move = chess.Move(from_sq, to_sq)\n",
    "            move_set.add(move.uci())\n",
    "            \n",
    "            from_rank = chess.square_rank(from_sq) # Get the row in which the piece is coming from\n",
    "            to_rank = chess.square_rank(to_sq) # Get the row in which will be moved the piece\n",
    "            # if to_rank in [0, 7] and from_rank in [1,6]:  # posibles promociones\n",
    "            if (from_rank == 1 and to_rank == 0) or (from_rank == 6 and to_rank ==7):\n",
    "                \n",
    "                for promo in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "                    move_set.add(chess.Move(from_sq, to_sq, promotion=promo).uci())\n",
    "                    \n",
    "    move_list = sorted(move_set)\n",
    "    uci_to_idx = {uci: idx for idx, uci in enumerate(move_list)}\n",
    "    idx_to_uci = {idx: uci for uci, idx in uci_to_idx.items()}\n",
    "    return uci_to_idx, idx_to_uci\n",
    "\n",
    "def fen_to_tensor(fen:str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a FEN position into a torch tensor of shape (12,8,8),\n",
    "    12 matrix of 8x8 positions, in which each type of piece eaither PNBRQK or pnbrqk,\n",
    "    will ocupate a place in the matrix, each matrix for each set of piece representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fen : str\n",
    "          The notation FEN to convert into numerical values\n",
    "    Returns\n",
    "    -------\n",
    "    board_tensor : torch.Tensor\n",
    "                   The representation of FEN notation in 12 matrix of 8x8\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    board = chess.Board(fen)\n",
    "    \n",
    "    piece_to_index = {piece:idx for idx,piece in enumerate('PNBRQKpnbrqk')} # represents the piece and index of each value of the str\n",
    "\n",
    "    #TODO: add extra ccanals to indicate if there is castling available 4 canals, passant square, halfmove clock\n",
    "    \n",
    "    board_tensor = torch.zeros((12,8,8),dtype=torch.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            idx = piece_to_index[piece.symbol()]\n",
    "            row = 7 - (square // 8)\n",
    "            col = square %8\n",
    "            board_tensor[idx,row,col] = 1.0\n",
    "    return board_tensor\n",
    "\n",
    "\n",
    "def get_legal_moves_vocab(fen:str) -> tuple[dict[str,int],dict[int,str]]:\n",
    "    \"\"\"\n",
    "    Generates a set of legal posible moves for a given position \n",
    "\n",
    "    IMPORTANT ---> All the dict generated are LOCAL and could not match with the global dict --> generate_full_uci_move_vocabulary()\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        fen: FEN notation of the current position\n",
    "    Returns\n",
    "    -------\n",
    "        uci_to_idx: Dict {uci_move : idx}\n",
    "        idc_to_uci: Dict {idx : uci_move}\n",
    "    \"\"\"\n",
    "\n",
    "    board = chess.Board(fen)\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    \n",
    "    legal_moves_sorted = sorted(legal_moves, key=lambda m: m.uci())\n",
    "\n",
    "    uci_to_idx = {move.uci():  idx for idx, move in enumerate(legal_moves_sorted)}\n",
    "    idx_to_uci = {idx: move.uci() for idx,move in enumerate(legal_moves_sorted)}\n",
    "    return uci_to_idx, idx_to_uci\n",
    "\n",
    " ## POSIBLEMENTE DESCARTADO, MEJORA ALTERNATIVA CON FUNCION  --> get_legal_moves_vocab enfoque \"SPARSE\"\n",
    "# def get_legal_mask(board: chess.Board, uci_to_index: dict) -> torch.Tensor:\n",
    "#     mask = torch.zeros(len(uci_to_index), dtype=torch.float32)\n",
    "#     for move in board.legal_moves:\n",
    "#         uci = move.uci()\n",
    "#         if uci in uci_to_index:\n",
    "#             mask[uci_to_index[uci]] = 1.0\n",
    "#     return mask  # Shape: (n_moves,)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    move_list = sorted(move_set)\n",
    "    uci_to_index = {uci: idx for idx, uci in enumerate(move_list)}\n",
    "    index_to_uci = {idx: uci for uci, idx in uci_to_index.items()}\n",
    "    return uci_to_index, index_to_uci\n",
    "# Globales cargados una vez al inicio\n",
    "\n",
    "def move_to_index(uci_move: str) -> int:\n",
    "    return uci_to_index.get(uci_move, -1)  # -1 si no está\n",
    "\n",
    "def index_to_move(idx: int) -> str:\n",
    "    return index_to_uci.get(idx, \"0000\")  # dummy por si acaso\n",
    "\n",
    "class ChessSequenceDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,df,uci_to_idx):\n",
    "\n",
    "        \n",
    "        self.games = []\n",
    "        self.uci_to_idx = uci_to_idx\n",
    "        grouped = df.groupby('game_id')\n",
    "\n",
    "        for game_id, group in grouped:\n",
    "            group_sorted = group.sort_values(by='ply',ascending=True)#group.sort_values(by='pyl',ascending=True)\n",
    "            sequence = []\n",
    "\n",
    "            for _,row in group_sorted.iterrows():\n",
    "                fen = row['fen']\n",
    "                uci = row['move_uci']\n",
    "\n",
    "                move_idx = uci_to_idx.get(uci,-1)\n",
    "                if move_idx ==-1:\n",
    "                    continue\n",
    "                try:\n",
    "                    fen_tensor = fen_to_tensor(fen)\n",
    "                except Exception as e:\n",
    "                    print(f'Error in FEN {fen} --->{e}')\n",
    "                    continue\n",
    "                sequence.append((fen_tensor,move_idx))\n",
    "            if len(sequence)>0:\n",
    "                self.games.append(sequence)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.games)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.games[idx]\n",
    "        \n",
    "def identity_collate(batch):\n",
    "    return batch[0]  # simplemente devuelve la secuencia tal cual\n",
    "\n",
    "class ChessLSTMPolicyNet(torch.nn.Module):\n",
    "    def __init__(self,input_channels=12,lstm_hidden_size=512,lstm_layers=1,output_dim=4544):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_channels,64,kernel_size=3,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.flattened_size = 128*8*8\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size= self.flattened_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc = torch.nn.Linear(lstm_hidden_size,output_dim)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "            B, T, C, H, W = x.shape\n",
    "            x = x.view(B * T, C, H, W)           # (B*T, C, 8, 8)\n",
    "            x = self.conv(x)                     # (B*T, 128, 8, 8)\n",
    "            x = x.view(B, T, -1)                 # (B, T, 8192)\n",
    "\n",
    "            lstm_out, _ = self.lstm(x)           # (B, T, hidden)\n",
    "            logits = self.fc(lstm_out)           # (B, T, output_dim)\n",
    "            return logits\n",
    "def train_lstm(model, dataloader, criterion, optimizer,device, epochs=5,start_epoch=0, checkpoint_path='checkpoint.pth'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(start_epoch,start_epoch+epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_moves = 0\n",
    "\n",
    "        for sequence in dataloader:\n",
    "            # batch: list of 1 element (sequence of (fen_tensor, move_idx))\n",
    "            if not all(isinstance(x,tuple) and isinstance(x[0],torch.Tensor) for x in sequence):\n",
    "                print('Invalid sequence detected and omitted...')\n",
    "                continue\n",
    "            try: \n",
    "                \n",
    "                inputs = torch.stack([x[0] for x in sequence])  # (T, C, 8, 8)\n",
    "                targets = torch.tensor([x[1] for x in sequence],dtype=torch.long)  # (T,)\n",
    "\n",
    "            # Reshape to (B, T, C, 8, 8)\n",
    "                inputs = inputs.unsqueeze(0).to(device)\n",
    "                targets = targets.unsqueeze(0).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)  # (B, T, output_dim)\n",
    "\n",
    "            # Aplanar para CrossEntropy\n",
    "                logits = outputs.view(-1, outputs.size(-1))     # (T, output_dim)\n",
    "                target_flat = targets.view(-1)                  # (T,)\n",
    "\n",
    "                loss = criterion(logits, target_flat)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                total_correct += (preds == target_flat).sum().item()\n",
    "                total_moves += target_flat.size(0)\n",
    "            except Exception as e:\n",
    "                print(f'Something went wrong error {e}, skipping sequence')\n",
    "                continue\n",
    "\n",
    "        acc = total_correct / total_moves\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {acc:.4f}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91e526-b54d-4b09-8859-55fc4351852b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa30ce-8b91-41bc-aa62-c0cd9b7f6349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f299c-fc9d-4a91-98c1-a26b4059d42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ebefe2-8a8c-41d4-8b3c-14201db2885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(parquets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b60e648-d560-4f6c-8863-b288817a80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids = list(df['game_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2d9860b-c97a-4061-895b-41e5392b8cd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_samples_id = np.random.choice(game_ids,size=50_000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7974f452-4cde-4dcf-afa2-26b6311dd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_samples_df = df.loc[df['game_id'].isin(game_samples_id)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b53e15-9367-4764-82a4-23bafa8e2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_to_idx,idx_to_uci = generate_full_uci_move_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f318793e-2422-4f22-ad36-6355520b8ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.utils' from '/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/__init__.py'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86d213a1-2627-489b-8b9f-bb1463660857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset y dataloader (batch_size = 1 por ahora)\n",
    "dataset = ChessSequenceDataset(game_samples_df, uci_to_idx)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True,collate_fn=identity_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0224099-e192-4123-8857-dea185099d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458/480673911.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Checkpoint cargado correctamente. Reanudando desde la época 5...\n",
      "Epoch 6: Loss = 161314.5505, Accuracy = 0.2409\n",
      "Epoch 7: Loss = 124684.6962, Accuracy = 0.3199\n",
      "Epoch 8: Loss = 115862.3961, Accuracy = 0.3503\n",
      "Epoch 9: Loss = 109969.8881, Accuracy = 0.3732\n",
      "Epoch 10: Loss = 105284.8164, Accuracy = 0.3925\n"
     ]
    }
   ],
   "source": [
    "# Define la clase del modelo si no está ya\n",
    "# class ChessLSTMPolicyNet(...): ...\n",
    "\n",
    "# Parámetros básicos\n",
    "checkpoint_path = 'checkpoint.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-3  # puedes ajustar esto\n",
    "\n",
    "# 1. Crear modelo y optimizador vacíos\n",
    "model = ChessLSTMPolicyNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# 2. Cargar checkpoint desde disco\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"[INFO] Checkpoint cargado correctamente. Reanudando desde la época {start_epoch}...\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[INFO] No se encontró checkpoint en {checkpoint_path}. Empezando desde cero.\")\n",
    "    start_epoch = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "706adbec-a45b-41c9-b2f8-1aeffbf2a767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 101203.2960, Accuracy = 0.4103\n",
      "Epoch 7: Loss = 97477.6327, Accuracy = 0.4271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Continuar entrenamiento\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_lstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# nuevas épocas adicionales\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 213\u001b[0m, in \u001b[0;36mtrain_lstm\u001b[0;34m(model, dataloader, criterion, optimizer, device, epochs, start_epoch, checkpoint_path)\u001b[0m\n\u001b[1;32m    210\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    212\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 213\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, T, output_dim)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Aplanar para CrossEntropy\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))     \u001b[38;5;66;03m# (T, output_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[21], line 187\u001b[0m, in \u001b[0;36mChessLSTMPolicyNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)                     \u001b[38;5;66;03m# (B*T, 128, 8, 8)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                 \u001b[38;5;66;03m# (B, T, 8192)\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# (B, T, hidden)\u001b[39;00m\n\u001b[1;32m    188\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out)           \u001b[38;5;66;03m# (B, T, output_dim)\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Continuar entrenamiento\n",
    "train_lstm(\n",
    "    model=model,\n",
    "    dataloader=loader,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    epochs=5,               # nuevas épocas adicionales\n",
    "    start_epoch=start_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdf52f-1764-4024-ad94-3fbbaed59f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess_ai_env",
   "language": "python",
   "name": "chess_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
