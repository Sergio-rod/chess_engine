{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f738d7b1-089b-4b95-8723-97fbba9e189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import chess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9a2093-8711-4726-8647-995475b26ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_uci_move_vocabulary() -> tuple[dict,dict]:\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a set of all the posible movements in a chess board of 64 squares, \n",
    "    create two dictionaries which will represent the board move in uci format and their respective idx value and viceversa\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        uci_to_idx : dict\n",
    "                     All the possible uci moves in a chess board, uci format as keys and idx as values\n",
    "        idx_to_uci : dict \n",
    "                     All the possible uci moves in a chess board, uci format as keys and idx as values\n",
    "    \n",
    "    \"\"\"\n",
    "    move_set = set()\n",
    "    \n",
    "    for from_sq in chess.SQUARES:\n",
    "        for to_sq in chess.SQUARES:\n",
    "            if from_sq == to_sq:\n",
    "                continue\n",
    "\n",
    "            move = chess.Move(from_sq, to_sq)\n",
    "            move_set.add(move.uci())\n",
    "            \n",
    "            from_rank = chess.square_rank(from_sq) # Get the row in which the piece is coming from\n",
    "            to_rank = chess.square_rank(to_sq) # Get the row in which will be moved the piece\n",
    "            # if to_rank in [0, 7] and from_rank in [1,6]:  # posibles promociones\n",
    "            if (from_rank == 1 and to_rank == 0) or (from_rank == 6 and to_rank ==7):\n",
    "                \n",
    "                for promo in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "                    move_set.add(chess.Move(from_sq, to_sq, promotion=promo).uci())\n",
    "                    \n",
    "    move_list = sorted(move_set)\n",
    "    uci_to_idx = {uci: idx for idx, uci in enumerate(move_list)}\n",
    "    idx_to_uci = {idx: uci for uci, idx in uci_to_idx.items()}\n",
    "    return uci_to_idx, idx_to_uci\n",
    "\n",
    "def fen_to_tensor(fen:str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a FEN position into a torch tensor of shape (12,8,8),\n",
    "    12 matrix of 8x8 positions, in which each type of piece eaither PNBRQK or pnbrqk,\n",
    "    will ocupate a place in the matrix, each matrix for each set of piece representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fen : str\n",
    "          The notation FEN to convert into numerical values\n",
    "    Returns\n",
    "    -------\n",
    "    board_tensor : torch.Tensor\n",
    "                   The representation of FEN notation in 12 matrix of 8x8\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    board = chess.Board(fen)\n",
    "    \n",
    "    piece_to_index = {piece:idx for idx,piece in enumerate('PNBRQKpnbrqk')} # represents the piece and index of each value of the str\n",
    "\n",
    "    #TODO: add extra ccanals to indicate if there is castling available 4 canals, passant square, halfmove clock\n",
    "    \n",
    "    board_tensor = torch.zeros((12,8,8),dtype=torch.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            idx = piece_to_index[piece.symbol()]\n",
    "            row = 7 - (square // 8)\n",
    "            col = square %8\n",
    "            board_tensor[idx,row,col] = 1.0\n",
    "    return board_tensor\n",
    "\n",
    "\n",
    "def get_legal_moves_vocab(fen:str) -> tuple[dict[str,int],dict[int,str]]:\n",
    "    \"\"\"\n",
    "    Generates a set of legal posible moves for a given position \n",
    "\n",
    "    IMPORTANT ---> All the dict generated are LOCAL and could not match with the global dict --> generate_full_uci_move_vocabulary()\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        fen: FEN notation of the current position\n",
    "    Returns\n",
    "    -------\n",
    "        uci_to_idx: Dict {uci_move : idx}\n",
    "        idc_to_uci: Dict {idx : uci_move}\n",
    "    \"\"\"\n",
    "\n",
    "    board = chess.Board(fen)\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    \n",
    "    legal_moves_sorted = sorted(legal_moves, key=lambda m: m.uci())\n",
    "\n",
    "    uci_to_idx = {move.uci():  idx for idx, move in enumerate(legal_moves_sorted)}\n",
    "    idx_to_uci = {idx: move.uci() for idx,move in enumerate(legal_moves_sorted)}\n",
    "    return uci_to_idx, idx_to_uci\n",
    "\n",
    " ## POSIBLEMENTE DESCARTADO, MEJORA ALTERNATIVA CON FUNCION  --> get_legal_moves_vocab enfoque \"SPARSE\"\n",
    "# def get_legal_mask(board: chess.Board, uci_to_index: dict) -> torch.Tensor:\n",
    "#     mask = torch.zeros(len(uci_to_index), dtype=torch.float32)\n",
    "#     for move in board.legal_moves:\n",
    "#         uci = move.uci()\n",
    "#         if uci in uci_to_index:\n",
    "#             mask[uci_to_index[uci]] = 1.0\n",
    "#     return mask  # Shape: (n_moves,)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    move_list = sorted(move_set)\n",
    "    uci_to_index = {uci: idx for idx, uci in enumerate(move_list)}\n",
    "    index_to_uci = {idx: uci for uci, idx in uci_to_index.items()}\n",
    "    return uci_to_index, index_to_uci\n",
    "# Globales cargados una vez al inicio\n",
    "\n",
    "def move_to_index(uci_move: str) -> int:\n",
    "    return uci_to_index.get(uci_move, -1)  # -1 si no estÃ¡\n",
    "\n",
    "def index_to_move(idx: int) -> str:\n",
    "    return index_to_uci.get(idx, \"0000\")  # dummy por si acaso\n",
    "\n",
    "class ChessSequenceDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,df,uci_to_idx):\n",
    "\n",
    "        \n",
    "        self.games = []\n",
    "        self.uci_to_idx = uci_to_idx\n",
    "        grouped = df.groupby('game_id')\n",
    "\n",
    "        for game_id, group in grouped:\n",
    "            group_sorted = group.sort_values(by='pyl',ascending=True)#group.sort_values(by='pyl',ascending=True)\n",
    "            sequence = []\n",
    "\n",
    "            for _,row in group_sorted.iterrows():\n",
    "                fen = row['fen']\n",
    "                uci = row['move_uci']\n",
    "\n",
    "                move_idx = uci_to_idx.get(uci,-1)\n",
    "                if move_idx ==-1:\n",
    "                    continue\n",
    "                try:\n",
    "                    fen_tensor = fen_to_tensor(fen)\n",
    "                except Exception as e:\n",
    "                    print(f'Error in FEN {fen} --->{e}')\n",
    "                    continue\n",
    "                sequence.append((fen_tensor,move_idx))\n",
    "            if len(sequence)>0:\n",
    "                self.games.append(sequence)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.games)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.games[idx]\n",
    "        \n",
    "def identity_collate(batch):\n",
    "    return batch[0]  # simplemente devuelve la secuencia tal cual\n",
    "\n",
    "class ChessLSTMPolicyNet(torch.nn.Module):\n",
    "    def __init__(self,input_channels=12,lstm_hidden_size=512,lstm_layers=1,output_dim=4544):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_channels,64,kernel_size=3,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.flattened_size = 128*8*8\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size= self.flattened_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc = torch.nn.Linear(lstm_hidden_size,output_dim)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "            B, T, C, H, W = x.shape\n",
    "            x = x.view(B * T, C, H, W)           # (B*T, C, 8, 8)\n",
    "            x = self.conv(x)                     # (B*T, 128, 8, 8)\n",
    "            x = x.view(B, T, -1)                 # (B, T, 8192)\n",
    "\n",
    "            lstm_out, _ = self.lstm(x)           # (B, T, hidden)\n",
    "            logits = self.fc(lstm_out)           # (B, T, output_dim)\n",
    "            return logits\n",
    "def train_lstm(model, dataloader, criterion, optimizer,device, epochs=5,start_epoch=0, checkpoint_path='checkpoint.pth'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(start_epoch,start_epoch+epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_moves = 0\n",
    "\n",
    "        for sequence in dataloader:\n",
    "            # batch: list of 1 element (sequence of (fen_tensor, move_idx))\n",
    "            if not all(isinstance(x,tuple) and isinstance(x[0],torch.Tensor) for x in sequence):\n",
    "                print('Invalid sequence detected and omitted...')\n",
    "                continue\n",
    "            try: \n",
    "                \n",
    "                inputs = torch.stack([x[0] for x in sequence])  # (T, C, 8, 8)\n",
    "                targets = torch.tensor([x[1] for x in sequence],dtype=torch.long)  # (T,)\n",
    "\n",
    "            # Reshape to (B, T, C, 8, 8)\n",
    "                inputs = inputs.unsqueeze(0).to(device)\n",
    "                targets = targets.unsqueeze(0).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)  # (B, T, output_dim)\n",
    "\n",
    "            # Aplanar para CrossEntropy\n",
    "                logits = outputs.view(-1, outputs.size(-1))     # (T, output_dim)\n",
    "                target_flat = targets.view(-1)                  # (T,)\n",
    "\n",
    "                loss = criterion(logits, target_flat)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                total_correct += (preds == target_flat).sum().item()\n",
    "                total_moves += target_flat.size(0)\n",
    "            except Exception as e:\n",
    "                print(f'Something went wrong error {e}, skipping sequence')\n",
    "                continue\n",
    "\n",
    "        acc = total_correct / total_moves\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {acc:.4f}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc91e526-b54d-4b09-8859-55fc4351852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_chunks(df, uci_to_idx,chunk_size=50_000, output_prefix='chunk_sequence'):\n",
    "    game_ids = df['game_id'].unique()\n",
    "    total_games = len(game_ids)\n",
    "    num_chunks = (total_games + chunk_size -1) // chunk_size\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i*chunk_size\n",
    "        end = min(start + chunk_size,total_games)\n",
    "        chunks_ids = game_ids[start:end]\n",
    "        chunk_df = df.loc[df['game_id'].isin(chunks_ids)]\n",
    "\n",
    "        print(f'Processing chunk number {i}/{num_chunks} with {len(chunks_ids)} games...')\n",
    "        dataset = ChessSequenceDataset(chunk_df,uci_to_idx)\n",
    "        save_path = Path.cwd().parent /'data'/'torch_datasets'/f'{output_prefix}_{i:04d}.pt'\n",
    "        save_path.parent.mkdir(parents=True,exist_ok=True)\n",
    "        torch.save(dataset.games,save_path)\n",
    "        print(f'[ð¾] Saved: {save_path} with {len(dataset)} valid games')\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0224099-e192-4123-8857-dea185099d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define la clase del modelo si no estÃ¡ ya\n",
    "# # class ChessLSTMPolicyNet(...): ...\n",
    "\n",
    "# # ParÃ¡metros bÃ¡sicos\n",
    "# checkpoint_path = 'checkpoint.pth'\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# lr = 1e-3  # puedes ajustar esto\n",
    "\n",
    "# # 1. Crear modelo y optimizador vacÃ­os\n",
    "# model = ChessLSTMPolicyNet().to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# # 2. Cargar checkpoint desde disco\n",
    "# try:\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     start_epoch = checkpoint['epoch'] + 1\n",
    "#     print(f\"[INFO] Checkpoint cargado correctamente. Reanudando desde la Ã©poca {start_epoch}...\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"[INFO] No se encontrÃ³ checkpoint en {checkpoint_path}. Empezando desde cero.\")\n",
    "#     start_epoch = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f299c-fc9d-4a91-98c1-a26b4059d42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "706adbec-a45b-41c9-b2f8-1aeffbf2a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Continuar entrenamiento\n",
    "# train_lstm(\n",
    "#     model=model,\n",
    "#     dataloader=loader,\n",
    "#     criterion=torch.nn.CrossEntropyLoss(),\n",
    "#     optimizer=optimizer,\n",
    "#     device=device,\n",
    "#     epochs=5,               # nuevas Ã©pocas adicionales\n",
    "#     start_epoch=start_epoch\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b779900-6496-4517-8729-c909c5d58987",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets = glob.glob(str(Path.cwd().parent / 'data' / 'processed'/'*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ebefe2-8a8c-41d4-8b3c-14201db2885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_games = pd.concat([pd.read_parquet(parquet) for parquet in parquets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a51c71-37e0-4439-92d6-599e5d2f45c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uci_to_idx,idx_to_uci =generate_full_uci_move_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5505e996-85a5-49a5-a8de-5eba3bb67cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk number 0/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0000.pt with 20000 valid games\n",
      "Processing chunk number 1/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0001.pt with 20000 valid games\n",
      "Processing chunk number 2/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0002.pt with 20000 valid games\n",
      "Processing chunk number 3/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0003.pt with 20000 valid games\n",
      "Processing chunk number 4/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0004.pt with 20000 valid games\n",
      "Processing chunk number 5/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0005.pt with 20000 valid games\n",
      "Processing chunk number 6/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0006.pt with 20000 valid games\n",
      "Processing chunk number 7/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0007.pt with 20000 valid games\n",
      "Processing chunk number 8/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0008.pt with 20000 valid games\n",
      "Processing chunk number 9/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0009.pt with 20000 valid games\n",
      "Processing chunk number 10/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0010.pt with 20000 valid games\n",
      "Processing chunk number 11/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0011.pt with 20000 valid games\n",
      "Processing chunk number 12/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0012.pt with 20000 valid games\n",
      "Processing chunk number 13/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0013.pt with 20000 valid games\n",
      "Processing chunk number 14/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0014.pt with 20000 valid games\n",
      "Processing chunk number 15/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0015.pt with 20000 valid games\n",
      "Processing chunk number 16/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0016.pt with 20000 valid games\n",
      "Processing chunk number 17/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0017.pt with 20000 valid games\n",
      "Processing chunk number 18/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0018.pt with 20000 valid games\n",
      "Processing chunk number 19/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0019.pt with 20000 valid games\n",
      "Processing chunk number 20/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0020.pt with 20000 valid games\n",
      "Processing chunk number 21/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0021.pt with 20000 valid games\n",
      "Processing chunk number 22/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0022.pt with 20000 valid games\n",
      "Processing chunk number 23/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0023.pt with 20000 valid games\n",
      "Processing chunk number 24/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0024.pt with 20000 valid games\n",
      "Processing chunk number 25/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0025.pt with 20000 valid games\n",
      "Processing chunk number 26/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0026.pt with 20000 valid games\n",
      "Processing chunk number 27/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0027.pt with 20000 valid games\n",
      "Processing chunk number 28/51 with 20000 games...\n",
      "[ð¾] Saved: /mnt/c/Users/rodri/Documents/Career Learning/DataLab/chess_ai_project/data/torch_datasets/chunk_sequence_0028.pt with 20000 valid games\n",
      "Processing chunk number 29/51 with 20000 games...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_dataset_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43muci_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36msave_dataset_chunks\u001b[0;34m(df, uci_to_idx, chunk_size, output_prefix)\u001b[0m\n\u001b[1;32m     10\u001b[0m chunk_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(chunks_ids)]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing chunk number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m games...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChessSequenceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43muci_to_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m save_path \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch_datasets\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m save_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 134\u001b[0m, in \u001b[0;36mChessSequenceDataset.__init__\u001b[0;34m(self, df, uci_to_idx)\u001b[0m\n\u001b[1;32m    131\u001b[0m group_sorted \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyl\u001b[39m\u001b[38;5;124m'\u001b[39m,ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#group.sort_values(by='pyl',ascending=True)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m sequence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _,row \u001b[38;5;129;01min\u001b[39;00m group_sorted\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    135\u001b[0m     fen \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfen\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    136\u001b[0m     uci \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove_uci\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:606\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 606\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    608\u001b[0m         object_index\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    611\u001b[0m     ):\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    613\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1189\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mM8[ns]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mlib.pyx:2543\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/numeric.py:361\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    359\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[1;32m    360\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 361\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dataset_chunks(df_all_games,uci_to_idx,chunk_size=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a2faa-e6c4-4508-b684-ff16bba986da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b60e648-d560-4f6c-8863-b288817a80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids = list(df['game_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2d9860b-c97a-4061-895b-41e5392b8cd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_samples_id = np.random.choice(game_ids,size=50_000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7974f452-4cde-4dcf-afa2-26b6311dd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_samples_df = df.loc[df['game_id'].isin(game_samples_id)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b53e15-9367-4764-82a4-23bafa8e2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_to_idx,idx_to_uci = generate_full_uci_move_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f318793e-2422-4f22-ad36-6355520b8ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.utils' from '/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/__init__.py'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504138e-d00c-4730-8d6c-56ded3cab838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86d213a1-2627-489b-8b9f-bb1463660857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset y dataloader (batch_size = 1 por ahora)\n",
    "dataset = ChessSequenceDataset(game_samples_df, uci_to_idx)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True,collate_fn=identity_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffdf52f-1764-4024-ad94-3fbbaed59f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess_ai_env",
   "language": "python",
   "name": "chess_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
